# Prompts sourced from: https://github.com/ncsoft/offsetbias/blob/master/prompt/offsetbias.py
# Minimal changes are made to accommodate new info (e.g., rubrics and references)

PROMPT_PAIRWISE = """You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
                
Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.
Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY “Output (a)” or “Output (b)”. Do NOT output any other words.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better.

# Instruction:
{input}
# Output (a):
{output_1}
# Output (b):
{output_2}
# Which is better, Output (a) or Output (b)? Your response should be either “Output (a)” or “Output (b)”:"""

PROMPT_PAIRWISE_TIE = """You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
                
Select the Output (a) or Output (b) that is better for the given instruction. If the two outputs are equally good, select Tie. The two outputs are generated by two different AI chatbots respectively.
Do NOT provide any explanation for your choice.
You should answer using ONLY “Output (a)” or “Output (b)” or "Tie". Do NOT output any other words.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better.

# Instruction:
{input}
# Output (a):
{output_1}
# Output (b):
{output_2}
# Which is better, Output (a) or Output (b)? Your response should be either “Output (a)” or “Output (b)” or "Tie":"""

PROMPT_PAIRWISE_RUBRIC_REF = """You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
                
Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.
Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY “Output (a)” or “Output (b)”. Do NOT output any other words.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the responses satisfy the provided rubric. Then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) You should refer to the provided reference answer as a guide for evaluating the responses.
(3) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(4) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better.

# Instruction:
{input}
# Output (a):
{output_1}
# Output (b):
{output_2}
# Rubric:
{rubric}
# Reference answer:
{reference_answer}
# Which is better, Output (a) or Output (b)? Your response should be either “Output (a)” or “Output (b)”:"""

PROMPT_ABSOLUTE = """You are a helpful assistant in evaluating the quality of the output for a given instruction. Your goal is to score the output for the given instruction.
                
Assign a score that is an integer between 1 and 5 for the given instruction. The output is generated by an AI chatbot.
Do NOT provide any explanation for your choice.
You should answer using ONLY an integer between 1 and 5. Do NOT output any other words.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, a longer the output does not necessarily mean that the response should be scored higher.

# Instruction:
{instruction}
# Output:
{response}
# What is the score of the output? Your response should be an integer between 1 and 5:"""

PROMPT_ABSOLUTE_RUBRIC_REF = """You are a helpful assistant in evaluating the quality of the output for a given instruction. Your goal is to score the output for the given instruction.
                
Assign a score that is an integer between 1 and 5 for the given instruction. The output is generated by an AI chatbot.
Do NOT provide any explanation for your choice.
You should answer using ONLY an integer between 1 and 5. Do NOT output any other words.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the response satisfies the provided rubric. Then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) You should refer to the provided reference answer as a guide for evaluating the responses.
(3) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(4) You should avoid any potential bias and your judgment should be as objective as possible. For example, a longer the output does not necessarily mean that the response should be scored higher.

# Instruction:
{instruction}
# Output:
{response}
# Rubric:
{rubric}
# Reference answer:
{reference_answer}
# What is the score of the output? Your response should be an integer between 1 and 5:"""

def get_prediction(generation, flip=False):
    if "Output (a)" in generation:
        if not flip:
            return 1
        else:
            return 2
    elif "Output (b)" in generation:
        if not flip:
            return 2
        else:
            return 1
    else:
        return -1

def get_prediction_tie(generation, flip=False):
    if "Output (a)" in generation:
        if not flip:
            return 1
        else:
            return 2
    elif "Output (b)" in generation:
        if not flip:
            return 2
        else:
            return 1
    elif "Tie" in generation:
        return 3
    else:
        return -1